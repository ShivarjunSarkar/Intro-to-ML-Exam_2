---
title: "Intro to ML - Exam 2"
author: "Shivarjun Sarkar, Yash Warty, Sahil Natu and Vivek Mehendiratta"
date: "16/08/2021"
output: pdf_document
---

## Github: https://github.com/ShivarjunSarkar/Intro-to-ML-Exam_2/

## Problem 1: **Green Buildings**

First we will go ahead and setup the required libraries:

```{r, echo = FALSE,warning=FALSE}
library(tidyverse)
library(mosaic)
library(dplyr)
```

Loading in the Green Buildings data

```{r, echo = FALSE}
df = read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/greenbuildings.csv")
```

Dropping the NA values from the empl_gr column for use in later
calculations:

```{r, echo = FALSE}
data=df%>%drop_na(empl_gr)
data=df%>%drop_na(age)
```

### Point 1

Checking the analyst claim of low leasing rate having weird stuff going
on

```{r, echo = FALSE}
plot(data$leasing_rate,data$Rent,xlab="Leasing Rate",ylab="Rent")
title(main="Relationship between Occupancy and Rent")
```

Looking at the simple plot, it seems like there is a positive
correlation. There is not anything weird in the data and so we can use
all entries.

### Point 2

Logically, newer buildings should have higher rents, even if they are
not green. The analyst has not really considered this factor. Let us
understand the distribution of buildings in our data:

```{r, echo = FALSE}
ggplot(data)+geom_histogram(aes(x = age, y = stat(density),fill=factor(green_rating)),binwidth = 5,)+
labs(x = 'Age')+
labs(y = 'Density')+
labs(title= 'Age of the building - Green vs Non Green')+
labs(fill = 'Green OR Non Green')
```

We can see that most buildings older than 50 years are non-green, since
green ratings are a new phenomenon. Building on this point, checking for
differences in rent between Green and Non-Green across age groups:

```{r, echo = FALSE}
data$binnedage = cut(data$age,c(0,10,20,30,40,50,60,70,80,90,100,200))
binnedage_grouping = data %>%
  group_by(binnedage,green_rating)%>%
  summarize(mean_rent = mean(Rent),n=n())
ggplot(binnedage_grouping,aes(x=binnedage,y = mean_rent,fill=factor(green_rating)))+
  geom_bar(stat = 'identity', position = 'dodge')+
  labs(x = 'Range of Age of the building')+
  labs(y = 'Average Rent')+
  labs(title = 'Rents of Green vs Non Green building by age group')+
  labs(fill = 'Green OR Non Green')
```

The premium for non-green buildings is higher in the initial years, with
green buildings only commanding significantly higher rents after about
30 years. So the analyst calculation of recouping costs may be
incorrect.

### Point 3

Class A buildings should ideally command the highest rent, with Class C
having the lowest rents.Creating the Classifications by Class

```{r, echo = FALSE}
data$classes = ifelse(data$class_a == 1, 'A', ifelse(data$class_b == 1, 'B', 'C'))
```

Grouping by Class

```{r, echo = FALSE}
boxplot(data$Rent~data$classes,xlab = 'Class of Building', ylab = "Rent in Building")
```

From the above plot, we see that buildings in Class A do have higher
rents than those in Class B and C. Let us now check for differences in
rent between Green and Non Green buildings:

```{r, echo = FALSE}
classes_grouping = data %>%
  group_by(classes,green_rating)%>%
  summarize(mean_rent = mean(Rent),n=n())
ggplot(classes_grouping,aes(x=classes,y = mean_rent,fill=factor(green_rating)))+
  geom_bar(stat = 'identity', position = 'dodge')+
  labs(x = 'Class of the building')+
  labs(y = 'Average Rent')+
  labs(title = 'Rents of Green vs Non Green buildings by Class')+
  labs(fill = 'Green OR Non Green')
```

As we can see, Class A and B (better quality buildings) have higher
rents for Non-Green buildings and so there is no benefit to be derived
from a Green building.

### Point 4

Austin does get a lot of rain in the summer, so maybe tenants prefer
green buildings since these are built to the LEED code and will have
additional facilities like rain-water harvesting and lightning
resistors, that may keep them safe and prevent damage.

```{r, echo = FALSE}
data$binnedprec = cut(data$Precipitation,c(0,10,20,30,40,50,60))
binnedprec_grouping = data %>%
  group_by(binnedprec,green_rating)%>%
  summarize(mean_rent = mean(Rent),n=n())
ggplot(binnedprec_grouping,aes(x=binnedprec,y = mean_rent,fill=factor(green_rating)))+
  geom_bar(stat = 'identity', position = 'dodge')+
  labs(x = 'Range of Precipitation at Location')+
  labs(y = 'Average Rent')+
  labs(title = 'Rents of Green vs Non Green building by Local Precipitation')+
  labs(fill = 'Green OR Non Green')
```

Austin gets about 33-36 inches of rain every year. If we look at
buildings in that range, it does appear that green buildings have a
slight advantage over non-green counterparts. However, there does not
appear to be a significant difference to warrant investing the
additional money and taking longer to get returns on our investment.

### Point 5

Austin is a booming city especially for younger populations. So
employment growth may be a larger factor in rent determination rather
than green or non-green. The analyst did not consider this:

```{r, echo = FALSE}
data$binnedempl = cut(data$empl_gr,c(-100,0,5,10,15,20,25,40,55,70))
binnedempl_grouping = data %>%
  group_by(binnedempl,green_rating)%>%
  summarize(mean_rent = mean(Rent),n=n())
ggplot(binnedempl_grouping,aes(x=binnedempl,y = mean_rent,fill=factor(green_rating)))+
  geom_bar(stat = 'identity', position = 'dodge')+
  labs(x = 'Employment Growth Around Location')+
  labs(y = 'Average Rent')+
  labs(title = 'Rents of Green vs Non Green building by Employment Growth')+
  labs(fill = 'Green OR Non Green')
```

Employer growth appears to prove the analysts point of Green buildings
having higher rents, especially in a booming city like Austin.

### Conclusion

After checking for various parameters, the cons of investing extra money
in a green building, appear to outweigh the pros. So overall, there is
no strong case for developing green buildings yet.

## Problem 2: **Flights at ABIA**

### **Visual story telling part 2: flights at ABIA**

The flight data contains data for all flights that departed from and
arrived at the Austin airport. We will analyze this data to search for
patterns and answer some questions.

```{r Q2setup, echo = FALSE}
rm(list = ls())
library(dplyr)
library(stringr)
library(reshape2) 

df <- read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv", header = TRUE, sep = ",")
dim(df)
colnames(df)
```

The data consists of 99260 rows and 29 variables. The variable names
have been listed above.

```{r Q2_dataclean, echo = FALSE}

sapply(df,class)

head(df)
for (i in c(5,6,7,8,10)){
  df[,i] <- as.character(df[,i])
  print(class(df[,i]))
}
df$type <- ifelse(df$Origin=="AUS","dep","arr")
df$DepHour <- substr(str_pad(df$DepTime,width=4,side="left",pad="0"),1,2)
df$ArrHour <- substr(str_pad(df$ArrTime,width=4,side="left",pad="0"),1,2)
df$DepDelayCap <- ifelse(df$DepDelay<0,0,df$DepDelay)
df$ArrDelayCap <- ifelse(df$ArrDelay<0,0,df$ArrDelay)
```

### Busiest Month

We will begin by understanding the temporal distribution of flights
across the year to find the busiest month.\
Spring and Summer are the busiest times of the year at the Austin
airport with a maximum of about 9,000 flights handled in the month of
June.

```{r Q2_1, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
flights_by_month <- df %>% group_by(Month) %>% tally()
plot(flights_by_month$n, type="o",
        main = "Busiest Month", xlab = "Months", ylab = "Number of Flights")
```

### Busiest Weekday

Next up, we will have a look at the temporal distribution of flights
across all weeks to find the busiest weekday. All 5 working days, Monday
to Friday are equally busy at the airport with a fall in flights on
Saturday and a slight bounce back on Sunday.

```{r Q2_2, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
flights_by_weekday <- df %>% group_by(DayOfWeek) %>% tally()
plot(flights_by_weekday$n, type="o",
     main = "Busiest Weekday", xlab = "Weekdays", ylab = "Number of Flights")
```

### Most Popular Airline

Next up, we look at distribution of flights across various carriers to
find the most popular airlines.\
Southwest Airlines (WN) is the most popular airlines with over 30,000
flights in 2008. This is followed by American Airlines (AA) and
Continental Airlines (CO).

```{r Q2_3, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
flight_by_carriers <- df %>% group_by(UniqueCarrier) %>% tally()
barplot(flight_by_carriers$n, names.arg = flight_by_carriers$UniqueCarrier,
        main = "Most Popular Flights", xlab = "Airlines", ylab = "Number of Flights")
```

### Departure Delays due to Bad Weather

We will analyze the data for delayed departures due to bad weather. We
start with checking for delay in departure due to bad weather across
multiple months.

```{r Q2_4_1, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
df_scen1 <- df[df$type=="dep" & df$Cancelled=="0",]
df_scen1$WeatherDelay[is.na(df_scen1$WeatherDelay)] <-  0
weatherdelay_by_month <- df_scen1 %>% group_by(Month) %>% summarize(WeatherDelay = mean(WeatherDelay))
plot(weatherdelay_by_month$WeatherDelay, type="o",
     main = "Avg Weather related Delays (min)", xlab = "Months", ylab = "Avg Delay in Mins")
```

While this provides us with average delay in minutes due to bad weather
across all months, we want to further see the frequency of weather
related delay occurrences in each month.

```{r Q2_4_2, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
weatherdelay_by_month$delayed <- df_scen1[df_scen1$WeatherDelay>0,] %>% group_by(Month) %>% tally()
weatherdelay_by_month$all <- df_scen1 %>% group_by(Month) %>% tally()
weatherdelay_by_month$delay_freq <- weatherdelay_by_month$delayed$n / weatherdelay_by_month$all$n
plot(weatherdelay_by_month$delay_freq*100, type="o",
     main = "Flights delayed by Bad Weather", xlab = "Months", ylab = "% of Flights Delayed")
```

We find similar results to the previous graph with a maximum delays
occurring in March. There is 1 weather related delay among every 65
flights in March. With March being the month with most weather related
delays, we check for frequency of delays in March.

```{r Q2_4_3, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
df_scen1$WeatherDelayInd <- ifelse(df_scen1$WeatherDelay>0,1,0)
weatherdelay_in_march <- df_scen1[df_scen1$Month==3,] %>% group_by(DayofMonth) %>% summarize(count = sum(WeatherDelayInd))
weatherdelay_in_march$all <- df_scen1[df_scen1$Month==3,] %>% group_by(DayofMonth) %>% tally()
weatherdelay_in_march$delay_freq <- weatherdelay_in_march$count / weatherdelay_in_march$all$n
plot(weatherdelay_in_march$delay_freq*100, type="o",
     main = "Flights delayed by Bad Weather in March", xlab = "Days of Month", ylab = "% of Flights Delayed")
```

Since flights were delayed for almost all days across the month (barring
a week at the end), the delays cannot be attributed to a standalone
weather event. Thus, we can conclude that March has the highest number
of weather related delays at the Austin airport. We can further break
down weather related delays in March by time of the day to figure out
flights at which hours are most likely to be affected by bad weather.

```{r Q2_4_4, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
weatherdelay_in_march_2 <- df_scen1[df_scen1$Month==3,] %>% group_by(DayofMonth,DepHour) %>% summarize(count = sum(WeatherDelayInd))
weatherdelay_in_march_2$all <- df_scen1[df_scen1$Month==3,] %>% group_by(DayofMonth,DepHour) %>%  tally()
weatherdelay_in_march_2$delay_freq <- weatherdelay_in_march_2$count / weatherdelay_in_march_2$all$n
weatherdelay_in_march_2$DepHour <- as.numeric(weatherdelay_in_march_2$DepHour)
weatherdelay_in_march_2_mat <- as.matrix(recast(weatherdelay_in_march_2[,c(1,2,5)], DayofMonth ~ DepHour, id.var = c("DayofMonth", "DepHour")))
weatherdelay_in_march_2_mat[is.na(weatherdelay_in_march_2_mat)] <- 0
heatmap(weatherdelay_in_march_2_mat[,-1], Rowv=NA, Colv=NA,main="Delays by hour and day in March",xlab="Hour of the Day",ylab="Day of the Month")
```

We can infer from the heat map that majority of the weather related
delays in March have happened before noon and none have happened during
the night time. We can plot a similar heat map for all months.

When looking at data for all the months, we still see a trend of no
weather related delays during the night hours with delays concentrated
in the morning hours right before noon and in the late evening hours.

```{r Q2_4_5, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
weatherdelay_by_month_2 <- df_scen1 %>% group_by(Month,DepHour) %>% summarize(count = sum(WeatherDelayInd))
weatherdelay_by_month_2$all <- df_scen1 %>% group_by(Month,DepHour) %>%  tally()
weatherdelay_by_month_2$delay_freq <- weatherdelay_by_month_2$count / weatherdelay_by_month_2$all$n
weatherdelay_by_month_2$DepHour <- as.numeric(weatherdelay_by_month_2$DepHour)
weatherdelay_by_month_2_mat <- as.matrix(recast(weatherdelay_by_month_2[,c(1,2,5)], Month ~ DepHour, id.var = c("Month", "DepHour")))
weatherdelay_by_month_2_mat[is.na(weatherdelay_by_month_2_mat)] <- 0
heatmap(weatherdelay_by_month_2_mat[,-1], Rowv=NA, Colv=NA,main="Delays by hour and month",xlab="Hour of the Day",ylab="Month")
```

### Flight Cancellations

Here, we begin by looking at flight cancellations by various carriers.

```{r Q2_5_1, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
can_by_carriers <- df %>% group_by(UniqueCarrier) %>% summarize(total_cancellations = sum(as.numeric(Cancelled)), avg_cancellations = mean(as.numeric(Cancelled)))
barplot(can_by_carriers$total_cancellations, names.arg = can_by_carriers$UniqueCarrier,
        main = "Carriers with Most Cancellations", xlab = "Airlines", ylab = "Number of Cancelled Flights")
```

American Airlines (AA) has the dubious distinction of being the carrier
with most cancelled flights. However, this is an incomplete picture
since all carriers may not be flying as many flights in and out of
Austin as American. Hence, we look at cancellation frequency among the
carriers.

```{r Q2_5_2, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
barplot(can_by_carriers$avg_cancellations*100, names.arg = can_by_carriers$UniqueCarrier,
        main = "Carriers with Most Frequent Cancellations", xlab = "Airlines", ylab = "% of Flights Cancelled")
```

It is now apparent that Envoy Air (MQ) has the highest frequency of
cancellations with 1 in every 16 of its flights getting cancelled.
Another important metric to look at when studying flight cancellations
is frequency of cancellation on routes that do not have more than 1
daily connection. A cancelled flight on such a route means waiting at
least a day before the next flight. We will call such routes as
*infrequent routes*.

```{r Q2_5_3, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
df$Route <- str_c(df$Origin,'-',df$Dest)
routes_agg <- df %>% group_by(Month,DayofMonth,Route,Origin,Dest) %>% tally()
routes_agg <- routes_agg[routes_agg$n <= 1,]
infrequent_routes <- routes_agg$Route
df_infrequent_routes <- df[df$Route %in% infrequent_routes,]
can_by_carriers_2 <- df_infrequent_routes %>% group_by(UniqueCarrier) %>% summarize(total_cancellations = sum(as.numeric(Cancelled)), avg_cancellations = mean(as.numeric(Cancelled)))
barplot(can_by_carriers_2$total_cancellations, names.arg = can_by_carriers_2$UniqueCarrier,
        main = "Carriers with Most Cancellations on Infrequent Routes", xlab = "Airlines", ylab = "Number of Cancelled Flights")
```

Again, American Airlines (AA) pops up as the carrier with most cancelled
flights. We further look at carrier wise flight cancellation frequency
for infrequent routes.

American Airlines (AA) and Endeavor Air (9E) are carriers with highest
cancellation frequencies for infrequent routes. Surprisingly, Envoy Air
(MQ), which had the worst cancellation rate overall, does not feature
here as it does not serve any infrequent route. ExpressJet Airlines (EV)
and Northwest Airlines (NW) have had no cancellations serving infrequent
routes, thus these carriers can be relied upon when traveling to or from
uncommon destinations.

```{r Q2_5_4, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
barplot(can_by_carriers_2$avg_cancellations*100, names.arg = can_by_carriers_2$UniqueCarrier,
        main = "Carriers with Most Frequent Cancellations on Infrequent Routes", xlab = "Airlines", ylab = "% of Flights Cancelled")
```

### Normalized Delay

Let us now look at Normalized Delay (with respect to total flight time)
across multiple variables. Not all delays are equal. A 30-minute delay
on a flight to Anchorage is not the same as a 30-minute delay on a short
flight like Austin to Dallas or Austin to Houston. We will normalize the
delay by dividing it by total flight duration. This gives us delay per
unit route duration.

*Note: We calculate total normalized delay as sum of normalized
departure and arrival delays. While these 2 delays cannot be added
directly as a delay in departure will inadvertently lead to delayed
arrival, we still proceed with this formula to penalize flights that
were not able to compensate their delayed departure in-flight as
compared to those flights which had a delayed departure but were able to
compensate for the delay by arriving before the estimated time of
arrival based on the route flight time.*

```{r Q2_6_1, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
df$NormalizedArrDelay = ifelse(is.na(df$ArrDelayCap/df$ActualElapsedTime),0,df$ArrDelayCap/df$ActualElapsedTime)
df$NormalizedDepDelay = ifelse(is.na(df$DepDelayCap/df$ActualElapsedTime),0,df$DepDelayCap/df$ActualElapsedTime)
df$NormalizedTotalDelay = df$NormalizedArrDelay + df$NormalizedDepDelay
delay_by_carriers = df %>% group_by(UniqueCarrier) %>% summarize(avg_total_delay = mean(NormalizedTotalDelay), avg_dep_delay = mean(NormalizedDepDelay), avg_arr_delay = mean(NormalizedArrDelay))
barplot(delay_by_carriers$avg_total_delay*60, names.arg = delay_by_carriers$UniqueCarrier,
        main = "Carriers with Highest Normalized Delay", xlab = "Airlines", ylab = "Avg Normalized Delay in seconds")
```

As evident from the bar plot, Envoy Air (MQ) has the highest normalized
delays to the tune of 20 seconds for every 1 minute of flight time. US
Airways (US) has the least delays. We can further look at routes with
the most amount of normalized delays, to and from Austin.

```{r Q2_6_2_a, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
delay_by_routes = df %>% group_by(Route,Origin,Dest,type) %>% summarize(avg_total_delay = mean(NormalizedTotalDelay), avg_dep_delay = mean(NormalizedDepDelay), avg_arr_delay = mean(NormalizedArrDelay))
```

**1)** Routes with highest delays flying out of Austin\
The plot throws up Austin to Des Moines as the most delayed route with a
delay of 2 minutes for every 1 minute of flight time. This looks fishy,
and hence upon further inspection, it turns out that the data has just 1
flight that has flown this route, and the extraordinary delay belongs to
this flight. Since we cannot generalize with such a small data sample,
Austin to Houston is the next most delayed route with high number of
routine flights and an average delay of 20 seconds for every 1 minute of
flight time.

```{r Q2_6_2_b, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
delay_by_routes_dep <- delay_by_routes[delay_by_routes$type == "dep",][order(delay_by_routes[delay_by_routes$type == "dep",]$avg_total_delay, decreasing=TRUE),]
barplot(delay_by_routes_dep$avg_total_delay[1:5]*60, names.arg = delay_by_routes_dep$Route[1:5],
        main = "Top 5 Austin Departures with Highest Normalized Delay", xlab = "Routes", ylab = "Avg Normalized Delay in seconds")
```

**2)** Routes with highest delays flying into Austin\
As we can see, flights from McGhee Tyson Airport (Knoxville,TN) have the
highest delays averaging over 1 minute of delay for every 1 minute of
flight time. However, there were only 3 flights from this airport to
Austin in the year 2008, and hence the sample is too small to generalize
that route as the most delayed at arrival in Austin. Hence, we look at
the next route from Oklahoma City to Austin. Flights from Oklahoma City
to Austin are delayed by an average of 40 seconds for every 1 minute of
flight time. Thus, this is the most delayed route coming in to Austin.\
Oklahoma City and Houston have featured in both the plots and thus we
can say with confidence that flights to and from Oklahoma City and
Houston are the most delayed relative to their flight times.

```{r Q2_6_2_c, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
delay_by_routes_arr <- delay_by_routes[delay_by_routes$type == "arr",][order(delay_by_routes[delay_by_routes$type == "arr",]$avg_total_delay, decreasing=TRUE),]
barplot(delay_by_routes_arr$avg_total_delay[1:5]*60, names.arg = delay_by_routes_arr$Route[1:5],
        main = "Top 5 Austin Arrivals with Highest Normalized Delay", xlab = "Routes", ylab = "Avg Normalized Delay in seconds")
```

We can further look at normalized delays across various months.\
The average normalized delays are the highest in December and March,
i.e. during the holiday season and in Spring.

```{r Q2_6_3, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
delay_by_months = df %>% group_by(Month) %>% summarize(avg_total_delay = mean(NormalizedTotalDelay), avg_dep_delay = mean(NormalizedDepDelay), avg_arr_delay = mean(NormalizedArrDelay))
barplot(delay_by_months$avg_total_delay*60, names.arg = delay_by_months$Month,
        main = "Months with Highest Normalized Delay", xlab = "Months", ylab = "Avg Normalized Delay in seconds")
```

### Answering some questions for travelers:

### a) Which carriers could be avoided during the Holiday Season?

Any flight that gets cancelled or delayed during the Holiday Season puts
a damper on one's spirits. Hence a traveler might want to avoid such
flights. We specifically look for cancellations and/or delays between
December 20 and January 10 to answer this question.

```{r Q2_7_1, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
df_holiday <- df[(df$Month==12 & df$DayofMonth>=20) | (df$Month==1 & df$DayofMonth<=10),]
holiday_avoid <- df_holiday %>% group_by(UniqueCarrier) %>% summarize(avg_normalized_delay = mean(NormalizedTotalDelay), total_cancellations = sum(as.numeric(Cancelled)))
holiday_avoid$all <- df_holiday %>% group_by(UniqueCarrier) %>% tally()
holiday_avoid$can_freq <- holiday_avoid$total_cancellations / holiday_avoid$all$n
barplot(holiday_avoid$avg_normalized_delay*60, names.arg = holiday_avoid$UniqueCarrier,
        main = "Normalized Delays during Holiday Season", xlab = "Airlines", ylab = "Avg Normalized Delay in seconds")
```

Continental Airlines (CO) has the highest delays, an average of over 25
seconds per 1 minute of flight time. Northwest Airlines (NW) has the
least average delay. The observations are different from those we
obtained earlier when we had considered the data for all months. Envoy
Air (MQ) has been the worst performing carrier overall, but that has
been replaced by Continental Airlines (CO) during the holiday season,
followed by Southwest Airlines (WN).

Upon looking at flight cancellations, it is evident that Envoy Air (MQ)
has the highest cancellations at 5% of their total scheduled flights,
followed by United Airlines (UA). Thus some of the carriers to avoid
during the holiday season are Continental Airlines, Southwest Airlines,
Envoy Air and United Airlines. One can expect a smoother start or end to
their holiday travels by hopping onto a flight operated by Northwest
Airlines, US Airways or ExpressJet Airlines.

```{r Q2_7_2, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
barplot(holiday_avoid$can_freq*100, names.arg = holiday_avoid$UniqueCarrier,
        main = "Frequency of Flight Cancellations during Holiday Season", xlab = "Airlines", ylab = "% of Flights Cancellled")
```

### b) Which overnight flights could be avoided?

Overnight flights or red-eye flights are the ones that have a
substantial part of their journey during the night hours. Travelers
generally prefer these flights to avoid spending precious daylight hours
in traveling. Any delay in these flights not only throws off the plans,
but also causes sleep deprivation as a traveler might be expecting to
sleep during these hours in the flight. We look at departure delays in
flights that depart between 11:00 PM and 3:59 AM to answer this
question.

```{r Q2_8_1, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
df_overnight <- df[(as.numeric(df$DepHour)>=23 | as.numeric(df$DepHour)<=3),]
overnight_avoid <- df_overnight %>% group_by(UniqueCarrier) %>% summarize(avg_dep_delay = mean(NormalizedDepDelay))
barplot(overnight_avoid$avg_dep_delay*60, names.arg = overnight_avoid$UniqueCarrier,
        main = "Normalized Departure Delays during Overnight Flights", xlab = "Airlines", ylab = "Avg Normalized Delay in seconds")
```

Envoy Air (MQ) and Southwest Airlines (WN) have the highest normalized
delays at departure for overnight flights. Departure delays in overnight
flights are a cause of concern irrespective of the duration of the
flight that is to be boarded. Thus, we also need to look at absolute
departure delays in overnight flights and not just the normalized
delays.

With absolute delays considered, Northwest Airlines (NW) is the worst
performing airline with highest average delay per flight at 5 hours,
followed by JetBlue Airways (B6) and Frontier Airlines (F9). The best
performing airlines for this metric are Mesa Airlines (YV), American
Airlines (AA), and Continental Airlines (CO). Envoy Air (MQ) and
Southwest Airlines (WN) still show up average delays on the higher side
(\~4 hours).

Thus, the carriers to avoid on overnight flights are Northwest Airlines,
JetBlue Airways, Frontier Airlines, Southwest Airlines and Envoy Air.

```{r Q2_8_2, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
df$DepDelayCap <- ifelse(is.na(df$DepDelayCap),0,df$DepDelayCap)
overnight_avoid$abs <- df_overnight %>% group_by(UniqueCarrier) %>% summarize(avg_dep_delay = mean(DepDelayCap))
barplot(overnight_avoid$abs$avg_dep_delay, names.arg = overnight_avoid$UniqueCarrier,
        main = "Absolute Departure Delays during Overnight Flights", xlab = "Airlines", ylab = "Avg Delay in minutes")
```

### c) Which flights could be avoided if you are flying out of Austin at the end of the day?

Many travelers take a flight out of a city at the end of the day to save
on accommodation costs for the night. A person can spend the entire day
in the city and fly out in late evening hours. However, if such a flight
is cancelled, the traveler seldom has options apart from staying the
night in the city, which entails additional costs for accommodation over
and above their planned trip budget. To answer this question, we will be
looking at flight cancellations data for departures from Austin between
7:00 PM and 10:59 PM.

```{r Q2_9_1, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
df_evening <- df[((as.numeric(df$DepHour)>=19 & as.numeric(df$DepHour)<=22) & df$Origin=="AUS"),]
evening_avoid <- df_evening %>% group_by(UniqueCarrier) %>% summarize(total_cancellations = sum(as.numeric(Cancelled)))
evening_avoid$all <- df_evening %>% group_by(UniqueCarrier) %>% tally()
evening_avoid$can_freq <- evening_avoid$total_cancellations / evening_avoid$all$n
barplot(evening_avoid$total_cancellations*100, names.arg = evening_avoid$UniqueCarrier,
        main = "Cancellation Frequency on Flights flying out of Austin in the EVening", xlab = "Airlines", ylab = "% of Flights Cancelled")
```

It turns out that no flights flying out of Austin during these hours
were cancelled in 2008. To further broaden our problem, we will look at
delays on flights flying out of Austin during these hours. Again, we
will look at absolute departure delay and not the normalized departure
delay as any delay, irrespective of the flight-time upon boarding, is a
cause of concern when flying out of Austin at the end of the day.\
\
Endeavor Air (9E) has the highest average delay of about 4 and a half
hours per flight when flying out of Austin in late evening. This is
followed by Delta Air Lines (DL). These carriers could be avoided when
flying out of Austin at the end of the day.

```{r Q2_9_2, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
evening_avoid$delay <- df_evening %>% group_by(UniqueCarrier) %>% summarize(avg_dep_delay = mean(DepDelay))
barplot(evening_avoid$delay$avg_dep_delay, names.arg = evening_avoid$UniqueCarrier,
        main = "Absolute Departure Delays for Flights flying out of Austin in the Evening", xlab = "Airlines", ylab = "Delay in minutes")
```

### d) Which flights to avoid if flying into Austin for work in the morning?

Many professionals that work in Austin fly into the city during the
morning hours. Any cancellations or delays on these flights would be
detrimental to the objectives of these flyers. Thus, we look at the data
for cancellations and delays on flights arriving into Austin between
8:00 AM and 10:59 AM.

```{r Q2_10_1, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
df$ArrDelayCap <- ifelse(is.na(df$ArrDelayCap),0,df$ArrDelayCap)
df_morning <- df[((as.numeric(df$ArrHour)>=7 & as.numeric(df$ArrHour)<=10) & df$Dest=="AUS"),]
morning_avoid <- df_morning %>% group_by(UniqueCarrier) %>% summarize(avg_arr_delay = mean(ArrDelayCap), total_cancellations = sum(as.numeric(Cancelled)))
barplot(morning_avoid$avg_arr_delay, names.arg = morning_avoid$UniqueCarrier,
        main = "Absolute Arrival Delays for Flight flying into Austin in the Morning", xlab = "Airlines", ylab = "Delay in minutes")
```

PSA Airlines (OH) has the highest average delay of 8 minutes on flights
arriving into Austin in the Morning. This is followed by Envoy Air
(MQ).\
\
No flights flying into Austin in the Morning hours were cancelled in
2008. Thus, based on delays alone, professionals can avoid flying PSA
Airlines and Envoy Air when flying into Austin in the Morning hours.
However, the delays here are not as significant as those seen during
other hours of the day.

```{r Q2_10_2, echo=FALSE, fig.align='left', fig.height = 4, fig.width = 10}
morning_avoid$all <- df_morning %>% group_by(UniqueCarrier) %>% tally()
morning_avoid$can_freq <- morning_avoid$total_cancellations / morning_avoid$all$n
barplot(morning_avoid$total_cancellations*100, names.arg = morning_avoid$UniqueCarrier,
        main = "Cancellation Frequency on Flights flying into Austin in the Morning", xlab = "Airlines", ylab = "% of Flights Cancelled")
```

## **Problem 3: Portfolio Modeling**

In this problem, we will construct three different portfolios of
exchange-traded funds, or ETFs, and use bootstrap resampling to analyze
the short-term tail risk of our portfolios.

```{r, echo=FALSE}
library(ggstance)
library(mosaic)
library(quantmod)
library(foreach)
```

We selected the ETFs ensuring diversity and different levels of risk.

Invesco QQQ is an exchange-traded fund that tracks the Nasdaq-100
IndexT. The Index includes the 100 largest non-financial companies
listed on the Nasdaq. It is one of the largest, owns only non-financial
stocks and is tech-heavy. The stock had performed well in 2017 but had a
poor return in 2018.

The SPDR S&P 500 trust is an exchange-traded fund which trades on the
NYSE Arca under the symbol. SPDR is an acronym for the Standard & Poor's
Depositary Receipts, the former name of the ETF. It is designed to track
the S&P 500 stock market index. SPY is one of the safest and largest
ETFs around.

SVXY is the ProShares Short VIX Short-Term Futures ETF, which provides
investors exposure to short VIX futures contracts. Put simply, investors
who buy SVXY are short S&P 500 volatility futures. It is a high risk
ETF. This is an unusual ETF since the performance is dependent on the
market volatility, not security.

ProShares UltraShort FTSE Europe (EPV) is a low performing ETF for the
past few years. iShares Core Growth Allocation ETF (AOR) is a very
diverse ETF. The iShares Core Growth Allocation ETF seeks to track the
investment results of an index composed of a portfolio of underlying
equity and fixed income funds intended to represent a growth allocation
target risk strategy.

YYY is a portfolio of 45 closed-end funds (CEFs) based on a rules based
index. This investment approach results in a portfolio which contains a
variety of asset classes, investment strategies and asset managers. The
index seeks to measure the performance of the top 45 U.S.
exchange-listed closed-end funds.

In total, we have selected 6 ETFs - "QQQ", "SPY", "SVXY", "EPV", "AOR"
and "YYY". We have considered 5 years of ETF data starting from
01-Jan-2014.

```{r, echo=FALSE}

# Import a few stocks
mystocks = c("QQQ", "SPY", "SVXY", "EPV", "AOR", "YYY")

# Getting the price data for 5 years
getSymbols(mystocks, from='2014-01-01')

#Loop through stocks and parse data
for(ticker in mystocks){
  expr = paste0(ticker, "a=adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

```

Sample Data for YYY

```{r, echo=FALSE}
head(YYYa)
```

```{r, echo=FALSE}
# Computing the return matrix
all_returns = cbind( ClCl(QQQa),
                     ClCl(SPYa),
                     ClCl(SVXYa),
                     ClCl(EPVa),
                     ClCl(AORa),
                     ClCl(YYYa))
# Remove NAs
all_returns = as.matrix(na.omit(all_returns))
head(all_returns)

```

Lets look at how the stocks are performing relative to each other. We
can see a strong correlation here. But it is complex and non-linear. As
discussed above, a few are performing well, others are not.

```{r, echo=FALSE}

# checking the correlation
pairs(all_returns)

```

Volatility of the ETFs across the 5 year period.

```{r, echo=FALSE}
# Volatility check
plot(ClCl(QQQa), type='l')
plot(ClCl(SPYa), type='l')
plot(ClCl(SVXYa), type='l')
plot(ClCl(EPVa), type='l')
plot(ClCl(AORa), type='l')
plot(ClCl(YYYa), type='l')
```

We can clearly observe while some stocks are highly volatile while some
are more stable in nature hence resulting in a balanced overall
selection

```{r, echo=FALSE}
initial_wealth = 100000
```

Our initial wealth is \$100,000

**SIMULATION 1 : SAFE Portfolio**

ETFs: "QQQ", "SPY", "SVXY", "EPV", "AOR", "YYY"

For the safe portfolio, we distributed 90% of the total wealth among the
high performing ETFs - QQQ, SPY and AOR.

```{r, echo=FALSE}

sim1 = foreach(i=1:5000, .combine = rbind) %do% {
  
  weights = c(0.3, 0.4, 0.03, 0.03, 0.2, 0.04)
  total_wealth = initial_wealth
  holdings = total_wealth * weights
  n_days = 20
  wealthtracker = rep(0, n_days)
  
  for(today in 1:n_days){
    return_today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings * (1 + return_today)
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    
    # Rebalancing
    holdings = total_wealth * weights
  }
  
  wealthtracker
}
```

```{r, echo = FALSE}
head(sim1)
hist(sim1[,n_days], 50)
plot(density(sim1[,n_days]))

# Profit/loss
hist(sim1[,n_days]- initial_wealth, breaks=30)
conf_5Per = confint(sim1[,n_days]- initial_wealth, level = 0.90)$'5%'
cat('\nAverage return of investement after 20 days', mean(sim1[,n_days]), "\n")
cat('\n5% Value at Risk for safe portfolio-',conf_5Per, "\n")
```

```{r, echo=FALSE}
wealth_daywise = c()
  
for (i in 1:n_days){
    wealth_daywise[i] = mean(sim1[,i]) 
}
days = 1:n_days
df = data.frame(wealth_daywise, days)
```

```{r, echo=FALSE}
ggplot(data=df, aes(x=days, y=wealth_daywise, group=1)) +
  geom_line(color="red")+
  geom_point() +
  xlab('Days') +
  ylab('Return of investments') + 
  ggtitle('Safe Portfolio: Retruns over 20 days')
```

```{r, echo=FALSE}
hist(sim1[,n_days], 50)
plot(density(sim1[,n_days]))
hist(sim1[,n_days]- initial_wealth, breaks=30)
conf_5Per = confint(sim1[,n_days]- initial_wealth, level = 0.90)$'5%'
cat('\nAverage return of investement after 20 days', mean(sim1[,n_days]), "\n")
cat('\n5% Value at Risk for safe portfolio-',conf_5Per, "\n")
```

**SIMULATION 2 : HIGH RISK PORTFOLIO**

For the high risk portfolio, we distributed 90% of the total wealth
among the low performing ETFs - SVXY, EPV and YYY.

Average return of investement after 20 days - \$100724.3\
5% Value at Risk for safe portfolio - \$7850.127

```{r, echo=FALSE}
sim2 = foreach(i=1:5000, .combine = rbind) %do% {
  weights = c(0.01, 0.02, 0.3, 0.3, 0.07, 0.3)
  total_wealth = initial_wealth
  holdings = total_wealth * weights
  n_days = 20
  wealthtracker = rep(0, n_days)
  
  for(today in 1:n_days){
    
    return_today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings * (1 + return_today)
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    
    # Rebalancing
    holdings = total_wealth * weights
  }
  
  wealthtracker
}

```

```{r, echo = FALSE}
head(sim2)
hist(sim2[,n_days], 50)
plot(density(sim2[,n_days]))

# Profit/loss
hist(sim2[,n_days]- initial_wealth, breaks=30)
hist(sim2[,n_days]- initial_wealth, breaks=30)
conf_5Per = confint(sim2[,n_days]- initial_wealth, level = 0.90)$'5%'
cat('\nAverage return of investement after 20 days', mean(sim2[,n_days]), "\n")
cat('\n5% Value at Risk for High portfolio-',conf_5Per, "\n")
```

```{r, echo=FALSE}
wealth_daywise = c()
  
for (i in 1:n_days){
    wealth_daywise[i] = mean(sim2[,i]) 
}
days = 1:n_days
df = data.frame(wealth_daywise, days)
```

```{r, echo=FALSE}
ggplot(data=df, aes(x=days, y=wealth_daywise, group=1)) +
  geom_line(color="red")+
  geom_point() +
  xlab('Days') +
  ylab('Return of investments') + 
  ggtitle('High Risk Portfolio: Retruns over 20 days')
```

```{r, echo=FALSE,}
hist(sim2[,n_days], 50)
plot(density(sim2[,n_days]))

# Profit/loss
hist(sim2[,n_days]- initial_wealth, breaks=30)
conf_5Per = confint(sim2[,n_days]- initial_wealth, level = 0.90)$'5%'
cat('\nAverage return of investement after 20 days', mean(sim2[,n_days]), "\n")
cat('\n5% Value at Risk for High portfolio-',conf_5Per, "\n")
```

**SIMULATION 3 - With equal weights for high risk and low risk**

```{r, echo=FALSE}
sim3 = foreach(i=1:5000, .combine = rbind) %do% {
  weights = c(0.17, 0.17, 0.17, 0.17, 0.17, 0.15)
  total_wealth = initial_wealth
  holdings = total_wealth * weights
  n_days = 20
  wealthtracker = rep(0, n_days)
  
  for(today in 1:n_days){
    
    return_today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings * (1 + return_today)
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    
    # Rebalancing
    holdings = total_wealth * weights
  }
  
  wealthtracker
}
head(sim3)
```

```{r, echo=FALSE}
hist(sim3[,n_days], 50)
plot(density(sim3[,n_days]))

# Profit/loss
hist(sim3[,n_days]- initial_wealth, breaks=30)
conf_5Per = confint(sim3[,n_days]- initial_wealth, level = 0.90)$'5%'
cat('\nAverage return of investement after 20 days', mean(sim3[,n_days]), "\n")
cat('\n5% Value at Risk for High portfolio-',conf_5Per, "\n")
```

```{r, echo=FALSE}
wealth_daywise = c()
for (i in 1:n_days){
    wealth_daywise[i] = mean(sim3[,i]) 
}
days = 1:n_days
df = data.frame(wealth_daywise, days)
```

```{r, echo=FALSE}
ggplot(data=df, aes(x=days, y=wealth_daywise, group=1)) +
  geom_line(color="red")+
  geom_point() +
  xlab('Days') +
  ylab('Return of investments') + 
  ggtitle('Diverse Portfolio: Retruns over 20 days')
```

### **Summary**

For the safe portfolio, we are observing the maximum return of
investment and the lowest 5% VaR. As the portfolio risk increases, we
are able to witness the decrease in returns and increase in VaR value as
expected.

References: <https://www.bankrate.com/investing/best-etfs/>
<https://etfdb.com/compare/lowest-ytd-returns/>

## **Problem 4: Market Segmentation**

### *Market Segmentation*

```{r, echo = FALSE}
# required libraries
library(corrplot)
```

### Data import and Exploration

```{r, echo = FALSE}
mkt = read.csv('https://raw.githubusercontent.com/jgscott/STA380/master/data/social_marketing.csv')
```

```{r, echo = FALSE}
head(mkt)
```

```{r, echo = FALSE}
summary(mkt)
```

```{r, echo = FALSE}
colnames(mkt)
```

To perform cluster analysis on the data, I will be using K-Means
Clustering approach. Following columns shall be removed to perform
cluster analysis : \* *X* : Since it is unique value for each user,
doesn't make much sense to keep it \* *chatter* : Random values which
doesn't fit in any column. It won't lie in any segment. \* *adult* :
Target variable, no need in cluster analysis \* *spam* : Target variable

```{r, echo = FALSE}
drop_columns = c("X", "chatter", "adult", "spam", "uncategorized")
mkt_km = mkt[, !(names(mkt) %in% drop_columns)]
```

### Data Standardization

```{r, echo = FALSE}
set.seed(1)
mkt_scaled = scale(mkt_km)
scaled_mean = attr(mkt_scaled, "scaled:center")
scaled_sd = attr(mkt_scaled, "scaled:scale")
cat("Scaled Mean :\n", scaled_mean, "\n\n")
cat("Scaled SD :\n", scaled_sd)
```

```{r fig.width=15, fig.height=15}
corrplot(cor(mkt_km), method = "number")
```

```{r, echo = FALSE}
cr = cor(mkt_km)
cr[upper.tri(cr, diag=TRUE)] <- NA
cr = reshape2::melt(cr, na.rm=TRUE, value.name="corr")
```

```{r, echo = FALSE}
cr = cr %>% arrange(desc(corr))
head(cr, 10)
```

Let's see if there is any highly negatively correlated variables ?

```{r, echo = FALSE}
tail(cr, 10)
```

The data has almost positive correlation with personal_fitness and
health_nutrition being top correlated variables.

### Clustering

```{r, echo = FALSE}
#k-means clustering with 10 clusters
cl = kmeans(mkt_scaled, 10, nstart=25)
```

### Distribution of columns in each cluster :

```{r, echo = FALSE}
for(i in c(1:10)){
  a = mkt_scaled[which(cl$cluster == i),]
  cat("cluster No :", i, "\n")
  print(sort(colSums(a[, 2:ncol(a)]), decreasing = T)[0:5])
  cat("\n")
}
```

### FInding the optimal value of `k` for K-Means Clustering

```{r, echo = FALSE}
kmean_withinss = function(k) {
    cl = kmeans(mkt_scaled, k)
    return (cl$tot.withinss)
}
```

```{r, echo = FALSE}
kmean_withinss(2)
```

```{r, echo = FALSE}
# Setting maximum cluster 
max_k = 17
# Run algorithm over a range of k 
wss = sapply(2:max_k, kmean_withinss)
```

```{r, echo = FALSE}
# Create a data frame to plot the graph
elbow = data.frame(2:max_k, wss)
```

```{r, echo = FALSE}
# Plot the graph with gglop
ggplot(elbow, aes(x = X2.max_k, y = wss)) +
    geom_point() +
    geom_line() +
    scale_x_continuous(breaks = seq(1, 20, by = 1))
```

`Optimal K : 10`

```{r, echo = FALSE}
cl2 = kmeans(mkt_scaled, 10)
```

```{r, echo = FALSE}
cl2$size
```

```{r, echo = FALSE}
cl2$centers
```

```{r, echo = FALSE}
for(i in c(1:10)){
  a = mkt_scaled[which(cl$cluster == i),]
  cat("cluster No :", i, "\n")
  print(names(sort(colSums(a[, 2:ncol(a)]), decreasing = T))[1:10])
  cat("\n")
}
```

**Insights**

Some clusters turned out to be meaningful and informative, below are the
some categories which can be clubbed together :

-   **Cluster 1** contains categories like `personal_fitness`,
    `nutrition_health` and `outdoors` can be taken posts related to
    fitness.
-   **Cluster 2** contains categories like `parenting`, `food`,
    `school`, `sports`, `religion`, `crafts`. These can be considered as
    posts from educational institutions.\
-   **Cluster 3** contains categories like `travel`, `politics`,
    `computers`, `news` which clearly show that the posts belong to
    news.
-   **Cluster 6** contains categories like `home_and_garden`, `dating`,
    `travel`, `small_business`, `tv_film`,`music` can be related
    lifestyle posts.
-   **Cluster 8** contains categories like `online_gaming`,
    `college_uni`, `sports_playing` can be related college online ganing
    event.

## **Problem 5: Author attribution**

### **Problem**

Predicting the authorship of the articles in the C50test directory using
a model trained using the c50train directory in the Reuters C50 Corpus.
Describe the pre-processing and analysis pipeline in detail

### **Analysis**

### Step 1: Reading in the files from C50train and C50test directories

```{r, echo = FALSE,warning=FALSE}

library(tm)
library(tidyverse)
library(slam)
library(proxy)

#Wrapper function
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }


author_dirs_train = Sys.glob('data/ReutersC50/C50train/*')
author_dirs_test = Sys.glob('data/ReutersC50/C50test/*')

```

We have imported all the authors files for training into
`author_dirs_train`. Now we will clean these files.

```{r, cache= TRUE,message=FALSE, warning=FALSE, paged.print=FALSE, results="hide"}

# Rolling all directories together into a single corpus and getting Author names

#Train Data 
file_list_train = NULL
labels = NULL

for(author in author_dirs_train) 
{
  author_name = substring(author, first = 21)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_train = append(file_list_train, files_to_add)
  labels = append(labels, rep(author_name, length(files_to_add)))
}

#Read files using wrapper function
all_docs_train = lapply(file_list_train, readerPlain) 

#Getting rid of '.txt' from filename
names(all_docs_train) = file_list_train
names(all_docs_train) = sub('.txt', '', names(all_docs_train))


#Test Data 
file_list_test = NULL
labels = NULL

for(author in author_dirs_test) 
{
  author_name = substring(author, first = 29)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add)
  labels = append(labels, rep(author_name, length(files_to_add)))
}

#Read files using wrapper function
all_docs_test = lapply(file_list_test, readerPlain) 

#Getting rid of '.txt' from filename
names(all_docs_test) = file_list_test
names(all_docs_test) = sub('.txt', '', names(all_docs_test))

```

`all_docs_train` has all the files. `author_name` has the name of the
authors who wrote the corresponding files. `file_list_train` has the
full path of all the files.

### Step 2: Creating the Document Term Matrix and TF-IDF for training

```{r, echo = FALSE}


TFIDF_Transform <- function(all_docs, DTM_given) {
  
         corpus = VCorpus(VectorSource(all_docs))
         corpus = corpus %>% 
                  tm_map(content_transformer(tolower)) %>%  #lowercase
                  tm_map(content_transformer(removeNumbers)) %>% # remove numbers
                  tm_map(content_transformer(removePunctuation)) %>%  # remove punctuation
                  tm_map(content_transformer(stripWhitespace)) %>%  ## remove excess spaces
                  tm_map(content_transformer(removeWords), stopwords("en")) %>%  #Remove en stop-words
                  tm_map(content_transformer(removeWords), stopwords("SMART")) #Remove SMART stop-words
                  
                  
        
         #Check Train or Test
        if(missing(DTM_given)) 
          {
            #Create Document Term Matrix
            DTM_Matrix = DocumentTermMatrix(corpus)
          } 
        else 
          {
            DTM_Matrix = DocumentTermMatrix(corpus, control = list(dictionary=Terms(DTM_given)))
          }
         
        
        #Summary
        DTM_Matrix
        
        #Remove Sparse Terms
        DTM_Matrix = removeSparseTerms(DTM_Matrix, 0.95)
        
        #Summary - #We now have 660 terms versus ~32000 that were present before
        DTM_Matrix
        
        #Create TF-IDF Matrix
        TFIDF_Matrix = weightTfIdf(DTM_Matrix)
        
        return(list(DTM=DTM_Matrix, tfidf=TFIDF_Matrix))
        
}


#Training Data
tfidf_return <- TFIDF_Transform(all_docs_train)
DTM_train    <- tfidf_return$DTM
tfidf_train  <- tfidf_return$tfidf

#Testing Data
tfidf_return <- TFIDF_Transform(all_docs_test,DTM_train)
tfidf_test   <- tfidf_return$tfidf

```

```{r, cache= TRUE,message=FALSE, warning=FALSE, paged.print=FALSE, results="hide"}

library(dplyr)

# Creating training corpus and processing
my_corpus_train = Corpus(VectorSource(all_docs_train))

#Preprocessing
my_corpus_train = my_corpus_train %>% 
                  tm_map(content_transformer(tolower))  %>%                        #transform to lowercase
                  tm_map(content_transformer(removeNumbers)) %>%                   #remove numbers
                  tm_map(content_transformer(removePunctuation)) %>%               #remove punctuation
                  tm_map(content_transformer(stripWhitespace)) %>%                 #remove excess spaces
                  tm_map(content_transformer(removeWords), stopwords("SMART")) %>% #remove smrt stop-words
                  tm_map(content_transformer(removeWords), stopwords("en"))        #remove en stop-words

#Create Document Term Matrix
DTM_train= DocumentTermMatrix(my_corpus_train)
DTM_train# some basic summary statistics

#Removing Sparse Terms
DTM_train = removeSparseTerms(DTM_train, 0.95)
DTM_train # now ~ 660 terms (versus ~32000 before)

#Create Weight TfIdf
tfidf_train = weightTfIdf(DTM_train)

```

### Step 3: Creating the Document Term Matrix and TF-IDF for testing

NOTE: The words which aren't present in the training data are dropped
here from the test DTM.

```{r, cache= TRUE,message=FALSE, warning=FALSE, paged.print=FALSE, results="hide"}

my_corpus_test = Corpus(VectorSource(all_docs_test))

#Pre-processing
my_corpus_test =  my_corpus_test %>% 
                  tm_map(content_transformer(tolower))  %>%                        #transform to lowercase
                  tm_map(content_transformer(removeNumbers)) %>%                   #remove numbers
                  tm_map(content_transformer(removePunctuation)) %>%               #remove punctuation
                  tm_map(content_transformer(stripWhitespace)) %>%                 #remove excess spaces
                  tm_map(content_transformer(removeWords), stopwords("SMART")) %>% #remove smrt stop-words
                  tm_map(content_transformer(removeWords), stopwords("en"))        #remove en stop-words

#Create Document Term Matrix for Test - Specifying control to maintain same dim as train
DTM_test= DocumentTermMatrix(my_corpus_test, control = list(dictionary=Terms(DTM_train)))
DTM_test #Has the same 660 terms

#Create Weight TfIdf
tfidf_test = weightTfIdf(DTM_test)

```

`all_docs_train` has all the files. `author_name` has the name of the
authors who wrote the corresponding files. `file_list_train` has the
full path of all the files.

### Step 4: Summarizing the Term matrices by using PCA

**Dimensionality reduction**

Principal component analysis is used to (1) extract relevant features
from the huge set of variables (2) eliminate the effect of
multi-collinearity while not losing out on relevant information from the
correlated variables

```{r, cache= TRUE, echo = FALSE}


#Create Train & test Data Frame
tfidf_train <- as.matrix(tfidf_train)
tfidf_test  <- as.matrix(tfidf_test)

#Drop All zero columns
tfidf_train <- tfidf_train[,which(colSums(tfidf_train) != 0)] 
tfidf_test  <- tfidf_test[,which(colSums(tfidf_test) != 0)]

#Pick common columns
tfidf_train = tfidf_train[,intersect(colnames(tfidf_train),colnames(tfidf_test))]
tfidf_test  = tfidf_test[,intersect(colnames(tfidf_test),colnames(tfidf_train))]

# PCA on the TF-IDF weights
pc_author = prcomp(tfidf_train)
pc_author_test <- predict(pc_author, newdata = tfidf_test)
pc_author_test <- as.data.frame(pc_author_test)

#Plot Importance
pve = summary(pc_author)$importance[3,]
plot(pve)

```

Since we cant see much of an elbow, we will cut at 75 Here approximately
70 percent of the data is expalined

### Step 5: Exploring Classification Models

```{r, cache = TRUE, warning=FALSE, paged.print=FALSE}
set.seed(123)
n_cut = 75

#Create X & y using 75 as the cutoff for Train
X = pc_author$x[,1:n_cut]
y = sapply(strsplit(names(all_docs_train), "/"), "[", 4)

#Create X & y using 75 as the cutoff for Test
X_test = pc_author_test[,1:n_cut]
y_test = sapply(strsplit(names(all_docs_test), "/"), "[", 4)

#Combine X & y for Train & Validation
TrainSet <- cbind(as.data.frame(X),y)
ValidSet <- cbind(as.data.frame(X_test),y_test)
```

### Model 1: Boosting

```{r ,cache = TRUE, warning=FALSE, paged.print=FALSE, echo = FALSE}
library(gbm)

#Create Boosting Model
boost.author <- gbm(y ~.,data=TrainSet, n.trees =100 ,shrinkage = 0.01,distribution = 
                      "multinomial",interaction.depth=4)

#== checking accuracy on trainset
pred=as.data.frame(predict(boost.author,newdata =TrainSet,n.trees=100,type="response"))
pred_val = sub("*\\.[0-9]+", "", colnames(pred)[apply(pred,1,which.max)])
mean(pred_val== y )

#== checking accuracy on test data
pred=as.data.frame(predict(boost.author,newdata =ValidSet,n.trees=100,type="response"))
pred_val = sub("*\\.[0-9]+", "", colnames(pred)[apply(pred,1,which.max)])
mean(pred_val== y_test )
```

Boosting classifies the training data well, but is unable to classify
the test data.

### Model 2: Random Forest

```{r, warning=FALSE, cache= TRUE, paged.print=FALSE, echo = FALSE}
library(randomForest)

rffit <- randomForest(as.factor(y)~.,TrainSet,ntree=200)
mean(predict(rffit,TrainSet)== y )
mean(predict(rffit,ValidSet)== y_test )
```

Random Forest does a better job than classification as compared to
Boosting.

### Model 3: Naive Bayes

```{r, warning=FALSE, cache= TRUE, paged.print=FALSE, echo = FALSE}
library('e1071')
set.seed(123)

naiveb <- naiveBayes(as.factor(y)~.,data=TrainSet)

mean(predict(naiveb,TrainSet)== y )
mean(predict(naiveb,ValidSet)== y_test )
```

Naive Bayes has a low training & testing accuracy

### Model 4: K-NearestNeighbors

```{r, warning=FALSE, cache= TRUE, paged.print=FALSE, echo = FALSE}
library(class)
set.seed(123)

trn_pred=knn(as.data.frame(X),as.data.frame(X),y,k=1)
tst_pred=knn(as.data.frame(X),as.data.frame(X_test),y,k=1)

mean(trn_pred== y)
mean(tst_pred== y_test)
```

### **Model Comparison and Conclusion**

4 different classification techniques were used to predict the author
for the documents. The comparison of their results are as follows:

|                    | Boosting | Random Forest | Naive Bayes | KNN    |
|--------------------|----------|---------------|-------------|--------|
| Training Accuracy  | 85%      | 100%          | 66%         | 100%   |
| Test Accuracy      | 39%      | 50%           | 42%         | 43%    |
| Computational Time | Long     | Medium        | Short       | Medium |

-Random forest provides the best accuracy out of the four methods, with
a 50%. The other two methods provide lower accuracy ranging between
38-43%. -Multi-nomial logistic regression and other tree based methods
can also be used for the attribution. But we have chosen 4 for this
exercise.

```{r, echo = FALSE}
library(ggplot2)
comp<-data.frame("Model"=c("Boosting","Random Forest","Naive Bayes","KNN"), 
                 "Test.accuracy"=c(39,50,43,44))
comp
ggplot(comp,aes(x=Model,y=Test.accuracy))+geom_col()
```

## **Problem 6: Association Rules Mining**

```{r, echo = FALSE}
# required libraries
library(tidyverse)
library(ggplot2)
library(arules)
library(arulesViz)
```

### Data import and Exploration

```{r, echo = FALSE}
grocery = read.transactions('https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt', sep = ",", header = F)
```

```{r, echo = FALSE}
dim(grocery)
```

```{r, echo = FALSE}
str(grocery)
```

```{r, echo = FALSE}
head(grocery)
```

```{r}
arules::inspect(grocery[1:5])
```

```{r, echo = FALSE}
grocery@itemInfo$labels[1:20]
```

```{r, echo = FALSE}
which(grocery@itemInfo$labels == 'whole milk')
```

```{r, echo = FALSE}
grocery@itemInfo$labels[167]
```

```{r, echo = FALSE}
itemFrequency(grocery[, 1:10])
```

```{r, echo = FALSE}
# N = no. of transactions
# support(x) = frequency(x) / N
# support(X,Y) = freq(X,Y) / N ----> freq(X,Y) = count of XY
# confidence(x => y) = P(XY) / P(X) = freq(X,Y) / freq(X)
# confidence of buying Y when X is bought
# lift(X => Y) = Support(X,Y) / Support(X)*support(Y)
itemFrequencyPlot(grocery, support = 0.06)
```

## Association Rule Mining (Apriori)

```{r, echo = FALSE}
# Now run the 'apriori' algorithm
# Look at rules with support > .005 & confidence >.1 & length (# artists) <= 5
groceryrules = apriori(grocery,
                  parameter=list(support=.005,confidence=.1, minlen=2))
```

```{r, echo = FALSE}
arules::inspect(groceryrules[1:10])
```

```{r, echo = FALSE}
arules::inspect(sort(groceryrules, by='lift')[1:20])
```

## Visualizing Apriori

```{r, echo = FALSE}
plot(groceryrules)
```

```{r, echo = FALSE}
## Choose a subset
arules::inspect(subset(groceryrules, subset=lift > 3))
```

```{r, echo = FALSE}
arules::inspect(subset(groceryrules, subset=confidence > 0.5))
```

```{r, echo = FALSE}
# can swap the axes and color scales
plot(groceryrules, measure = c("support", "lift"), shading = "confidence")
```

```{r, echo = FALSE}
# "two key" plot: coloring is by size (order) of item set
plot(groceryrules, method='two-key plot')
```

```{r, echo = FALSE}
# can now look at subsets driven by the plot
arules::inspect(subset(groceryrules, support > 0.03))
```

```{r, echo = FALSE}
sub1 = subset(groceryrules, subset=confidence > 0.01 & support > 0.005)
plot(sub1, method='graph')
```

```{r, echo = FALSE}
arules::inspect(subset(groceryrules, lhs %in% "ham"))
```

```{r, echo = FALSE}
arules::inspect(sort(subset(groceryrules, rhs %in% "whole milk"), by="confidence")[1:10])
```

```{r, echo = FALSE}
arules::inspect(subset(groceryrules, rhs %in% "berries"))
```

### **Insights**

-   `ham` has a very high lift value with `wheat bread`. Both products
    can be clubbed together as single entity or promotional discounts
    can be given if on both the products.

-   `whole milk` is a necessity and so the customer will likely purchase
    this by default. Hence, such products can be stored away from the
    main aisles. This will cause the customer to look for Milk around
    the supermarket and make him/her explore other new products in the
    process, increasing the total billed value per customer.

-   `berries` is often bought with `whipped sour cream`, thus can be
    placed together.

-   `hygine articles` and `napkins` are also bought together.

-   `citrus fruit`, `tropical fruit` and `pip fruit` are also bought
    together.

-   Weirdly `shopping bags`, `sausages` and `sliced cheese` are bought
    together, shopping bags can be placed nearby sausages and sliced
    cheese.
